{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first reducing of unnecessary nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST():\n",
    "    def __init__(self, directory):\n",
    "        self._directory = directory\n",
    "        \n",
    "        self._training_data = self._load_binaries(\"./mnist_data/train-images.idx3-ubyte\")\n",
    "        self._training_labels = self._load_binaries(\"./mnist_data/train-labels.idx1-ubyte\")\n",
    "        self._test_data = self._load_binaries(\"./mnist_data/t10k-images.idx3-ubyte\")\n",
    "        self._test_labels = self._load_binaries(\"./mnist_data/t10k-labels.idx1-ubyte\")\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        samples_n = self._training_labels.shape[0]\n",
    "        random_indices = np.random.choice(samples_n, samples_n // 10, replace = False)\n",
    "        np.random.seed()\n",
    "        \n",
    "        self._validation_data = self._training_data[random_indices]\n",
    "        self._validation_labels = self._training_labels[random_indices]\n",
    "        self._training_data = np.delete(self._training_data, random_indices, axis = 0)\n",
    "        self._training_labels = np.delete(self._training_labels, random_indices)\n",
    "    \n",
    "    def _load_binaries(self, file_name):\n",
    "        path = os.path.join(self._directory, file_name)\n",
    "        \n",
    "        with open(path, 'rb') as fd:\n",
    "            check, items_n = struct.unpack(\">ii\", fd.read(8))\n",
    "\n",
    "            if \"images\" in file_name and check == 2051:\n",
    "                height, width = struct.unpack(\">II\", fd.read(8))\n",
    "                images = np.fromfile(fd, dtype = 'uint8')\n",
    "                return np.reshape(images, (items_n, height, width))\n",
    "            elif \"labels\" in file_name and check == 2049:\n",
    "                return np.fromfile(fd, dtype = 'uint8')\n",
    "            else:\n",
    "                raise ValueError(\"Not a MNIST file: \" + path)\n",
    "    \n",
    "    \n",
    "    def get_training_batch(self, batch_size):\n",
    "        return self._get_batch(self._training_data, self._training_labels, batch_size)\n",
    "    \n",
    "    def get_validation_batch(self, batch_size):\n",
    "        return self._get_batch(self._validation_data, self._validation_labels, batch_size)\n",
    "    \n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self._get_batch(self._test_data, self._test_labels, batch_size)\n",
    "    \n",
    "    def _get_batch(self, data, labels, batch_size):\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "        \n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]\n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "    \n",
    "    \n",
    "    def get_sizes(self):\n",
    "        training_samples_n = self._training_labels.shape[0]\n",
    "        validation_samples_n = self._validation_labels.shape[0]\n",
    "        test_samples_n = self._test_labels.shape[0]\n",
    "        return training_samples_n, validation_samples_n, test_samples_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#STORING THE DATA\n",
    "mnist_data = MNIST('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 128, 191, 128, 128, 128,\n",
      "         128, 128,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 128, 255, 255, 255, 255,\n",
      "         255, 255, 255, 255, 255, 191, 191,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 128, 255, 191, 128, 128,\n",
      "         128, 128, 255, 255, 255, 255, 255, 191,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  64, 255,   0,   0,   0,\n",
      "           0,   0,   0,   0,  64, 128, 255, 255, 128,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 255, 255,  64,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 255, 191,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0, 128, 255, 128,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,  64, 255, 255,  64,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0, 128, 255,  64,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 128, 255, 191,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  64, 255, 191,  64,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0, 128, 255, 255,  64,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         128, 255, 255,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 191,\n",
      "         255, 191,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  64, 191, 255,\n",
      "         191,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 128, 255, 255, 191,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 128, 255, 255,  64,   0,\n",
      "           0,   0,   0,  64, 128, 128,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 255, 255, 191, 128, 128,\n",
      "         255, 255, 255, 255, 255, 191,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 128, 255, 255, 255, 255,\n",
      "         255, 255, 255, 191,  64,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  64, 191, 255, 255,\n",
      "         128,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]]], dtype=uint8), array([2], dtype=uint8))\n",
      "(array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 174, 253, 253,\n",
      "         253, 121, 121,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 173, 252, 252,\n",
      "         252, 253, 252, 241, 197,  23,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18,  40, 158,\n",
      "         158, 159, 247, 252, 252, 232, 198,  79,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  37,  89, 197, 252, 252, 246,  78,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,  17, 106, 252, 252,  93,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,  17, 176, 252, 252,  93,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,  15, 178, 252, 252, 203,  29,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  32,  83,\n",
      "          13,  27,  27, 180, 252, 252, 218,  29,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 160, 252,\n",
      "         196, 253, 252, 252, 248, 203,  84,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 160, 252,\n",
      "         252, 253, 252, 202,  82,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  98, 253,\n",
      "         253, 255, 139,  14,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 207,\n",
      "         252, 253,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  27,\n",
      "         252, 253,  62,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
      "         151, 253, 235,  22,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 106,   0,   0,   0,   0,\n",
      "          99, 253, 252, 131,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  20,  54, 158,  14,   0,   0,   0,\n",
      "           0, 253, 252, 158,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0, 164, 252, 252, 193,  82,  15,   0,\n",
      "          22, 253, 252, 158,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  35, 205, 246, 252, 252, 193, 160,\n",
      "         208, 253, 252, 158,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  84, 204, 248, 252, 252,\n",
      "         252, 253, 239,  80,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  82, 203, 252,\n",
      "         252, 190,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]]], dtype=uint8), array([3], dtype=uint8))\n",
      "(array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,  63, 204, 203,  19,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0, 188, 253, 252,  93,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 104, 246, 231, 106,  13,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0, 101, 246, 252,  75,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   4, 128, 253, 228,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          51,   0,   0,   0,  79, 252, 233,  72,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  76,\n",
      "         247, 116,   0,   0, 203, 252, 168,  13,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 237,\n",
      "         253, 133,   0, 101, 253, 177,  31,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  95, 178, 253,\n",
      "         126,  38,  48, 241, 255, 146,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  95, 225, 252, 177,\n",
      "          38,  38, 197, 252, 253, 234, 169, 169, 170,  82,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  89, 229, 252, 252, 177,\n",
      "         198, 209, 252, 252, 253, 252, 252, 252, 253, 196,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  76, 237, 253, 252, 252, 252,\n",
      "         253, 252, 252, 202, 140,  90, 139, 139, 128,  22,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  70, 253, 253, 251, 225, 225, 200,\n",
      "         255, 247, 150,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0, 119, 252, 202, 125,   0,  19, 231,\n",
      "         253, 171,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  38,  56,   6,   0,   0, 123, 252,\n",
      "         206,  56,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 246, 252,\n",
      "          63,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  79, 216, 216, 116,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  76, 216, 240, 109,   9,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  38, 213, 253, 109,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  38, 237, 128,   9,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]]], dtype=uint8), array([4], dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "#INVESTIGATING THE DATA\n",
    "for i, batch in enumerate(mnist_data.get_training_batch(1)):\n",
    "    print(batch)\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "#TRAINING PARAMS\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#INPUT\n",
    "input_channels = 1 #mnist is grey value \n",
    "input_size = 28 #mnist is 28x28 pics\n",
    "\n",
    "#CONVOLUTIONAL LAYER 1\n",
    "kernel_size_conv1 = 9\n",
    "stride_conv1 = 1\n",
    "channels_conv1 = 256\n",
    "\n",
    "#PRIMARY CAPSULES\n",
    "kernel_size_conv2 = 9\n",
    "dim_primary_caps = 8 #primary capsules are 8-D\n",
    "channels_primary_caps = 32\n",
    "no_neurons_primary_caps = 6*6*channels_primary_caps\n",
    "\n",
    "#DIGIT CAPSULE\n",
    "dim_digits_caps = 16 #capsules for digits are 16-D\n",
    "no_output_classes = 10 #mnist depicts 10 numbers from 0 to 9\n",
    "\n",
    "#RECONSTRUCTOR\n",
    "size_layer1 = 512\n",
    "size_layer2 = 1024\n",
    "size_layer3 = 28*28\n",
    "\n",
    "#LOSS\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lamb = 0.5\n",
    "reconstr_loss_factor = 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS GET WEIGHTS AND BIASES\n",
    "def get_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def get_biases(shape):\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR BUILDING THE NETWORK\n",
    "\n",
    "def conv_relu(x, weights, biases, stride):\n",
    "    conv = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding='VALID')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def conv(x, weights, biases, stride):\n",
    "    conv = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding='VALID')\n",
    "    return conv + biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#squashing function from paper\n",
    "def squash(tensor, axis):\n",
    "    \n",
    "    #tensor with same dimensions as the tensor with the length of the \n",
    "    #vector along the specified axis stored in every component of this\n",
    "    #vector, norm is the euclidean norm here\n",
    "           \n",
    "    norm = tf.norm(tensor, keep_dims=True, axis=axis)\n",
    "    normed_tensor = tensor/norm\n",
    "    \n",
    "    squashing_factor = norm**2/(1+norm**2)\n",
    "    \n",
    "    return squashing_factor * normed_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#routing algorithm \n",
    "def routing(prediction_vectors, iterations=3):\n",
    "    \n",
    "    #intitialize the logits b\n",
    "    #log priors can be learned ?!?!?!?!?\n",
    "    #for mnist they found its sufficient to set them to zero\n",
    "    b = tf.zeros(shape=[batch_size,\n",
    "                        no_neurons_primary_caps,\n",
    "                        no_output_classes]\n",
    "                )\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #compute the softmax \n",
    "        c = tf.nn.softmax(b)\n",
    "        c = tf.expand_dims(c, axis=-1)\n",
    "        c = tf.tile(c, [1, 1, 1, dim_digits_caps])\n",
    "        \n",
    "        #compute the input\n",
    "        s = tf.reduce_sum(c*prediction_vectors, axis=1)\n",
    "        \n",
    "        #compute the output\n",
    "        v = squash(s, axis=2)\n",
    "        \n",
    "        #compute the agreement\n",
    "        v_exp = tf.expand_dims(v, axis=1)\n",
    "        v_exp = tf.tile(v_exp, [1, no_neurons_primary_caps, 1, 1])\n",
    "        a = tf.reduce_sum(prediction_vectors*v_exp, axis=-1)\n",
    "        \n",
    "        #updating the logits\n",
    "        b = b+a\n",
    "        \n",
    "    return v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DESCIRBING THE DATAFLOW GRAPH\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images = tf.placeholder(dtype=tf.float32, shape=[batch_size,\n",
    "                                             input_size,\n",
    "                                             input_size])\n",
    "images_exp = tf.expand_dims(images, axis=-1)\n",
    "labels = tf.placeholder(dtype=tf.int64, shape=[batch_size])\n",
    "\n",
    "\n",
    "with tf.variable_scope('ReLU_Conv1'):\n",
    "    \n",
    "    weights = get_weights(shape = [kernel_size_conv1,\n",
    "                                    kernel_size_conv1,\n",
    "                                    input_channels,\n",
    "                                    channels_conv1]\n",
    "                            )\n",
    "   \n",
    "    biases = get_biases(shape = [channels_conv1])\n",
    "    \n",
    "    conv1 = conv_relu(images_exp, weights, biases, stride=1)\n",
    "    \n",
    "    \n",
    "\n",
    "with tf.variable_scope('Primary_Caps'):\n",
    "    \n",
    "    weights = get_weights(shape = [kernel_size_conv2,\n",
    "                                    kernel_size_conv2,\n",
    "                                    channels_conv1,\n",
    "                                    dim_primary_caps*channels_primary_caps]\n",
    "                         )\n",
    "    \n",
    "    biases = get_biases(shape = [dim_primary_caps*channels_primary_caps])\n",
    "    \n",
    "    #biases? relu? try to exclude it?\n",
    "    #paper doesnt state clear if relu is used. experiments online showed\n",
    "    # better results with relu, but Id say paper says not to use relu\n",
    "    primary_caps = conv(conv1, weights, biases, stride=2)\n",
    "    primary_caps = tf.reshape(primary_caps, shape=[batch_size,6,6,\n",
    "                                                   channels_primary_caps,\n",
    "                                                   dim_primary_caps]\n",
    "                             )\n",
    "    primary_caps = squash(primary_caps, axis=4)\n",
    "  \n",
    "\n",
    "    \n",
    "with tf.variable_scope('Digit_Caps'):\n",
    "    \n",
    "    weights = get_weights(shape = [1, no_neurons_primary_caps,\n",
    "                                   no_output_classes,\n",
    "                                   dim_primary_caps,\n",
    "                                   dim_digits_caps]\n",
    "                          )\n",
    "    \n",
    "    #get primary caps and weights into the \n",
    "    #right dims for matrix multiplication\n",
    "    primary_caps = tf.reshape(primary_caps, shape=[batch_size,\n",
    "                                                  no_neurons_primary_caps,\n",
    "                                                  1,\n",
    "                                                  dim_primary_caps,\n",
    "                                                  1])\n",
    "    primary_caps = tf.tile(primary_caps, [1, 1, 10, 1, 1])\n",
    "    weights = tf.tile(weights, [batch_size, 1, 1, 1, 1])\n",
    "    \n",
    "    #compute the prediction vectors\n",
    "    prediction_vectors = tf.matmul(weights, primary_caps, transpose_a=True)\n",
    "    prediction_vectors = tf.squeeze(prediction_vectors)\n",
    "    \n",
    "    digit_caps = routing(prediction_vectors)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    \n",
    "    length_digit_caps = tf.norm(digit_caps, axis = 2)\n",
    "    labels_one_hot = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    #relu for max(0, margin_loss)\n",
    "    plus_loss =  labels_one_hot * tf.nn.relu(m_plus - length_digit_caps)\n",
    "    minus_loss = lamb * (1 - labels_one_hot) * \\\n",
    "                        tf.nn.relu(length_digit_caps - m_minus)\n",
    "    \n",
    "    loss = tf.reduce_sum(plus_loss + minus_loss, axis=-1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(length_digit_caps, 1), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "#RECONSTRUCTOR\n",
    "\n",
    "    \n",
    "with tf.variable_scope('FC_ReLU_1'):\n",
    "    \n",
    "    #masking out all but the correct digit caps\n",
    "    labels_one_hot_exp = tf.expand_dims(labels_one_hot, axis=-1)\n",
    "    labels_one_hot_exp = tf.tile(labels_one_hot_exp, [1,1,16])\n",
    "    \n",
    "    masked_out_digit_caps = digit_caps * labels_one_hot_exp\n",
    "    \n",
    "    \n",
    "    weights = get_weights(shape=[dim_digits_caps * no_output_classes,\n",
    "                                 size_layer1\n",
    "                                ]\n",
    "                         )\n",
    "    biases = get_biases(shape=[size_layer1])\n",
    "    \n",
    "    digit_caps_flat = tf.reshape(digit_caps, shape=[batch_size,\n",
    "                                    dim_digits_caps * no_output_classes]\n",
    "                                )\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.matmul(digit_caps_flat, weights) + biases)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('FC_ReLU_2'):\n",
    "    \n",
    "    weights = get_weights(shape=[size_layer1, size_layer2])\n",
    "    biases = get_biases(shape=[size_layer2])\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
    "    \n",
    "\n",
    "with tf.variable_scope('FC_Sigmoid'):\n",
    "    \n",
    "    weights = get_weights(shape=[size_layer2, size_layer3])\n",
    "    biases = get_biases(shape=[size_layer3])\n",
    "    \n",
    "    fc3 = tf.nn.sigmoid(tf.matmul(fc2, weights) + biases)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_step = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#SUMMARIES\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3534440994262695, Accuracy: 0.1328125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-23a2ccdfaddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                     training_step],\n\u001b[1;32m     15\u001b[0m                                     feed_dict = {images: x,\n\u001b[0;32m---> 16\u001b[0;31m                                                 labels: y})\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_summaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luke/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luke/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luke/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luke/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luke/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_writer = tf.summary.FileWriter(\"./summaries/train\", tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    step = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        batch_generator = mnist_data.get_training_batch(batch_size)\n",
    "        \n",
    "        for x, y in batch_generator:\n",
    "            _loss, _accuracy, _summaries, _ = sess.run([loss,\n",
    "                                    accuracy,\n",
    "                                    merged_summaries, \n",
    "                                    training_step],\n",
    "                                    feed_dict = {images: x,\n",
    "                                                labels: y})\n",
    "            train_writer.add_summary(_summaries, step)\n",
    "            step += 1\n",
    "            print(\"Loss: {}, Accuracy: {}\".format(_loss, _accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
