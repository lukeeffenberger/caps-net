{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do the dimensions of the capsules represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block only prepares everything (import modules, load the data, define the data flow graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luke/CogSci/Master/ANN/caps-net/layers/convcapslayer.py:78: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from layers.capslayer import CapsLayer\n",
    "from layers.convcapslayer import ConvCapsLayer\n",
    "from layers.convlayer import ConvLayer\n",
    "from layers.denselayer import DenseLayer\n",
    "from wrappers.mnisthelper import MNIST\n",
    "\n",
    "# Loading the data.\n",
    "mnist_data = MNIST('./mnist_data/')\n",
    "\n",
    "# Define the functions needed for the data flow graph\n",
    "def mask_and_flatten_digit_caps(digit_caps, labels):\n",
    "    \"\"\"Mask out and flat the digit caps.\n",
    "\n",
    "    All but the 16 values of the capsule corresponding to the correct label\n",
    "    are set to 0. Then the digit caps are reshaped from [batch_size, 10, 16] to\n",
    "    [batch_size, 10*16].\n",
    "    \"\"\"\n",
    "    # Create a tensor in the same shape as the digit caps with ones at the\n",
    "    # entries that are corresponding to the entries for the correct label\n",
    "    # and zero everywhere else.\n",
    "    labels = tf.one_hot(labels, depth=10)\n",
    "    labels = tf.expand_dims(labels, axis=-1)\n",
    "    labels = tf.tile(labels, [1,1,16])\n",
    "    # Mask out the digit caps.\n",
    "    masked_digit_caps = digit_caps * labels\n",
    "    # Read out the batch size from the labels.\n",
    "    batch_size = tf.shape(labels)[0]\n",
    "    # Flat the digit caps.\n",
    "    masked_and_flat = tf.reshape(masked_digit_caps, shape=[batch_size,10*16])\n",
    "    return masked_and_flat\n",
    "\n",
    "def calculate_loss_accuracy(digit_caps, labels):\n",
    "    \"\"\"Calculate the loss and the accuracy.\n",
    "\n",
    "    The loss implements the loss from the paper. For more information check our\n",
    "    report.\n",
    "    Accuracy is computed by taking the digit with \"longest\" capsule, meaning the\n",
    "    digit where the model is the most sure, that it is in the image, as the\n",
    "    prediction of the network.\n",
    "    \"\"\"\n",
    "    # Compute the length (euclidean norm) of the digit capsules.\n",
    "    length_digit_caps = tf.norm(digit_caps, axis = 2)\n",
    "    labels_one_hot = tf.one_hot(labels, depth=10)\n",
    "    # Compute the false negative part of the loss.\n",
    "    plus_loss =  labels_one_hot * tf.nn.relu(0.9 - length_digit_caps)\n",
    "    # Compute the fals positive part of the loss.\n",
    "    minus_loss = 0.5 * (1 - labels_one_hot) * tf.nn.relu(length_digit_caps - 0.1)\n",
    "    # Compute the loss from those two parts\n",
    "    loss = tf.reduce_sum(plus_loss + minus_loss, axis=-1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    # Compute accuracy by comparing indices of longest capusle to the labels.\n",
    "    correct_prediction = tf.equal(tf.argmax(length_digit_caps, 1), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return loss, accuracy\n",
    "\n",
    "def calculate_reconstruction_loss(reconstructions, images):\n",
    "    \"\"\"Calculate the loss of the reconstruction.\"\"\"\n",
    "    # As the reconstructor has a sigmoid read out layer scale pixel intensties\n",
    "    # of the images down to [0,1].\n",
    "    images = images/255.0\n",
    "    # Compute the sum squared error between reconstruction and original image.\n",
    "    squared_error = tf.squared_difference(reconstructions, images)\n",
    "    sum_squared_error = tf.reduce_sum(squared_error, axis=-1)\n",
    "    # Scale down the reconstruction loss to let it not dominate the loss.\n",
    "    reconstruction_loss = 0.0005 * tf.reduce_mean(sum_squared_error)\n",
    "    return reconstruction_loss\n",
    "    \n",
    "\n",
    "# Define the daata flow graph. \n",
    "# Reset the graph.\n",
    "tf.reset_default_graph()\n",
    "# Define the placeholders.\n",
    "image_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28])\n",
    "label_placeholder = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "# Read out the batch size from the placeholders.\n",
    "batch_size = tf.shape(image_placeholder)[0]\n",
    "\n",
    "# Convolutional layer.\n",
    "with tf.variable_scope('ReLU_Conv1'):\n",
    "    # Add fourth dimension (number of channels) to the images, because it is\n",
    "    # needed for convolution.\n",
    "    image_reshaped = tf.expand_dims(image_placeholder, axis=-1)\n",
    "    convolution = ConvLayer(\n",
    "                    kernel_size = 9,\n",
    "                    stride = 1,\n",
    "                    padding = 'VALID',\n",
    "                    channels = 256,\n",
    "                    activation_function = 'ReLU'\n",
    "                  )(image_reshaped)\n",
    "\n",
    "# Primary Caps layer. Does another convolution and cuts it into capsules.\n",
    "with tf.variable_scope('Primary_Caps'):\n",
    "    primary_caps = ConvCapsLayer(\n",
    "                        kernel_size = 9,\n",
    "                        stride = 2,\n",
    "                        padding = 'VALID',\n",
    "                        dimension = 8,\n",
    "                        channels = 32,\n",
    "                    )(convolution)\n",
    "\n",
    "# Digit Caps layer. Basically the readout layer, that decides if the network\n",
    "# recognized a certain digit or not.\n",
    "with tf.variable_scope('Digit_Caps'):\n",
    "    digit_caps = CapsLayer(\n",
    "                        count2 = 10,\n",
    "                        dim2 = 16,\n",
    "                        rout_iter = 3\n",
    "                 )(primary_caps)\n",
    "\n",
    "# Calculate the loss and the accuracy of the read out.\n",
    "with tf.variable_scope('Loss'):\n",
    "    loss, accuracy = calculate_loss_accuracy(digit_caps, label_placeholder)\n",
    "\n",
    "# Reconstructor that reconstructs the image from the representation of the\n",
    "# digit capsules. Consist of three dense layers.\n",
    "# Initialize a placeholder for the digit capsules to feed in the perturbed versions.\n",
    "digit_caps_placeholder = tf.placeholder(dtype=tf.float32, shape=[10, 16])\n",
    "with tf.variable_scope('Dense1'):\n",
    "    # Mask out (set to zero) all but the correct digit capsule and flatten\n",
    "    # the tensor for the dense layer.\n",
    "    digit_caps_flat = mask_and_flatten_digit_caps(digit_caps_placeholder, label_placeholder)\n",
    "    dense_1 = DenseLayer(\n",
    "                    n_out = 512,\n",
    "                    activation_function = 'ReLU'\n",
    "              )(digit_caps_flat)\n",
    "\n",
    "with tf.variable_scope('Dense2'):\n",
    "    dense_2 = DenseLayer(\n",
    "                    n_out = 1024,\n",
    "                    activation_function = 'ReLU'\n",
    "              )(dense_1)\n",
    "\n",
    "with tf.variable_scope('Dense3'):\n",
    "    dense_3 = DenseLayer(\n",
    "                    n_out = 28*28,\n",
    "                    activation_function = 'Sigmoid'\n",
    "              )(dense_2)\n",
    "    # Reshape the output of this layer to same shape as the original image,\n",
    "    # to obtain the reconstruction.\n",
    "    reconstructions = tf.reshape(dense_3, shape=[batch_size, 28, 28])\n",
    "\n",
    "# Calculate the loss of the reconstruction.\n",
    "with tf.variable_scope('Reconstruction_Loss'):\n",
    "    reconstruction_loss = calculate_reconstruction_loss(reconstructions, image_placeholder)\n",
    "\n",
    "# Set AdamOptimizer with default values (as described in the paper) as\n",
    "# optimizer. It minimizes the sum of the loss and the reconstruction loss.\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    total_loss = loss + reconstruction_loss\n",
    "    training_step = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now have a look at, how the reconstruction of an capsule changes for slight adjustments (adding or substracting a small delta) in a certain dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(capsules, label):\n",
    "    \"\"\"Perturb the different dimension of a capsule.\n",
    "    \n",
    "    Tweak each dimension with 0.05 intervals between -0.25 and 0.25.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the array out of two unnecessary lists.\n",
    "    capsules = capsules[0][0]\n",
    "    # Initialize list to save the peturbed capsules.\n",
    "    perturbed_caps = []\n",
    "    # For every dimension.\n",
    "    for dim in range(16):\n",
    "        # For eleven deltas in [-0.25, 0.25].\n",
    "        for delta in np.linspace(-0.25, 0.25, 11):\n",
    "            # Copy the capsules to not change the orginal ones.\n",
    "            d = np.copy(capsules)\n",
    "            # Add delta to the entry of the current dimension.\n",
    "            d[label,dim] += delta\n",
    "            # Save the perturbed capsules in list.\n",
    "            perturbed_caps.append(d)\n",
    "     # Split it up in to one list for each dimension.\n",
    "    perturbed_caps = [perturbed_caps[i:i+11] for i in range(0, len(perturbed_caps), 11)]     \n",
    "    return perturbed_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_digit_all_dimensions():\n",
    "    \"\"\"Plot reconstructions for perturbations of all dimension for a random digit.\n",
    "    \n",
    "    The digit is randomly chosen from the training set.\n",
    "    This function is to explore the reconstructions and \"meanings\" of the dimensions\n",
    "    on a large scale.\n",
    "    \"\"\"\n",
    "    # Initialize TensorFlow \"Saver\" to restore the weights.\n",
    "    saver = tf.train.Saver()\n",
    "    # Session to look at the reconstructions.\n",
    "    with tf.Session() as sess:\n",
    "        # Restore the weights from model.ckpt.\n",
    "        saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "\n",
    "        # Get random image sample for which to look at the perturbed versions.\n",
    "        generator = mnist_data.get_training_batch(1)\n",
    "        for x, y in generator:\n",
    "            image_sample = x\n",
    "            label_sample = y\n",
    "            break\n",
    "        # Compute the capsules for that image.\n",
    "        capsules = sess.run([digit_caps],feed_dict = {\n",
    "                                            image_placeholder: image_sample,\n",
    "                                            label_placeholder: label_sample\n",
    "                                          }\n",
    "                    )\n",
    "        # Compute the perturbed versions of this capsule.\n",
    "        perturbed_capsules = perturb(capsules, label_sample)\n",
    "\n",
    "\n",
    "        # For the lists corresponding to all perturbings for one dimension.\n",
    "        for j, dimension_list in enumerate(perturbed_capsules):\n",
    "            # Print the current dimension.\n",
    "            print('Dimension {}'.format(j+1))\n",
    "            # Initialize a figure.\n",
    "            fig = plt.figure(figsize = (15,10))\n",
    "            # For all capsules corresponding to a pertubing with delta in the current dimension.\n",
    "            for i, delta_capsule in enumerate(dimension_list):\n",
    "                # Obtain the reconstructions from the graph.\n",
    "                _reconstructions = sess.run([reconstructions],feed_dict = {\n",
    "                                                                digit_caps_placeholder: delta_capsule,\n",
    "                                                                image_placeholder: image_sample,\n",
    "                                                                label_placeholder: label_sample\n",
    "                                                              }\n",
    "                                   )\n",
    "                # Add the reconstruction as an suplot to the figure.\n",
    "                fig.add_subplot(1,11,i+1)\n",
    "                plt.imshow(_reconstructions[0][0], cmap='gray')\n",
    "                plt.axis('off')\n",
    "            # Show the plot.\n",
    "            plt.show()\n",
    "            \n",
    "def specific_digit_specific_dimension(digit, dimension):\n",
    "    \"\"\"Plot reconstructions for perturbations of certain dimension for a certain digits.\n",
    "    \n",
    "    The sample is randomly chosen from the training set, but is the specified digit.\n",
    "    This function is to validate assumptions above the \"meanin\" of a certain dimension\n",
    "    of a certain capsule.\n",
    "    \"\"\"\n",
    "    # Initialize TensorFlow \"Saver\" to restore the weights.\n",
    "    saver = tf.train.Saver()\n",
    "    # Session to look at the reconstructions.\n",
    "    with tf.Session() as sess:\n",
    "        # Restore the weights from model.ckpt.\n",
    "        saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "\n",
    "        # Get random image sample for which to look at the perturbed versions.\n",
    "        generator = mnist_data.get_training_batch(1)\n",
    "        for x, y in generator:\n",
    "            if y[0] == digit:\n",
    "                image_sample = x\n",
    "                label_sample = y\n",
    "                break\n",
    "        # Compute the capsules for that image.\n",
    "        capsules = sess.run([digit_caps],feed_dict = {\n",
    "                                            image_placeholder: image_sample,\n",
    "                                            label_placeholder: label_sample\n",
    "                                          }\n",
    "                    )\n",
    "        # Compute the perturbed versions of this capsule.\n",
    "        perturbed_capsules = perturb(capsules, label_sample)\n",
    "        # Extract only the specified dimension.\n",
    "        perturbed_capsules = perturbed_capsules[dimension-1]\n",
    "\n",
    "        # Print the specified digit and dimension.\n",
    "        print('Digit {}, Dimension {}'.format(digit, dimension))\n",
    "        # Initialize a figure.\n",
    "        fig = plt.figure(figsize = (15,10))\n",
    "        # For all capsules corresponding to a pertubing with delta in the current dimension.\n",
    "        for i, delta_capsule in enumerate(perturbed_capsules):\n",
    "            # Obtain the reconstructions from the graph.\n",
    "            _reconstructions = sess.run([reconstructions],feed_dict = {\n",
    "                                                            digit_caps_placeholder: delta_capsule,\n",
    "                                                            image_placeholder: image_sample,\n",
    "                                                            label_placeholder: label_sample\n",
    "                                                          }\n",
    "                               )\n",
    "            # Add the reconstruction as an suplot to the figure.\n",
    "            fig.add_subplot(1,11,i+1)\n",
    "            plt.imshow(_reconstructions[0][0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        # Show the plot.\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n",
      "Digit 8, Dimension 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f00623da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "specific_digit_specific_dimension(8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
